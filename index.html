<!DOCTYPE html>
<html lang="en">

<head>
	<link rel="stylesheet" href="styles/normalize.css">
	<link rel="stylesheet" href="styles/simple.css">
	<link rel="stylesheet" href="utils.css">
	<meta name="viewport" content="width=device-width, initial-scale=1">


</head>

<body>
	<header>
		<h2>Welcome to MNIST Battleground</h2>
	</header>

<!-- SECTION: NAVBAR -->
	<nav class="navbar">
		<ul>
			<li><a href="#intro">Intro</a></li>
			<li><a href="#methodology">Methodology</a></li>
			<li><a href="#activation_functions">Activation Functions</a></li>
			<li><a href="#about">About</a></li>
			<li><a href="#contact">Contact</a></li>
		</ul>
	</nav>
<!-- END: NAVBAR -->

	<main>

<!-- SECTION: INTRODUCTION -->
		<h3 id="intro" class="navTarget">Introduction</h3>
		<blockquote>
			<p>Deep Learning is full of choices:</p>
			<dl>
				<dd>What architecture to pick?</dd>
				<dd>What loss function?</dd>
				<dd>What learning rate?</dd>
				<dd>What activation functions?</dd>
				<dd>...and so on.</dd>
			</dl>
			<p>And many words have been spilt across the internet about all of these, but rarely are those words backed up with data.</p>
			<p><strong>That</strong> is what we're doing here.</p>
			<p>We're starting with a simple dataset that everyone should be familiar with: <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST</a>, and we'll be testing everything we can think of, and posting the results here. More data. Less fluff.</p>
			<p>In particular, we'll be presenting results in terms of Accuracy (out of 100% on the validation set) vs FLOPs spent on training. This later is a twist compared to the usual practice of counting training batches, but when different architecture choices may involve substantially different computational costs, it's important to have an apples-to-apples comparison.</p>
		</blockquote>
<!-- END: INTRODUCTION -->

<!-- SECTION: METHODOLOGY -->
		<article id="methodology" class="navTarget">
			<h3>Methodology</h3>
			<p><strong>Philosophy:</strong> Our philosophy is to start with the simplest and easiest things we can do first, and iterate from there.</p>
			<p><strong>FLOPs:</strong> We report our results in terms of FLOPs spent on training—rather than batch number—because it gives a more fair comparison between different model architectures. We obtain our FLOPs numbers by using the <a href="https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html">PyTorch Profiler</a> tool, which produces FLOPs estimates that so far have landed spot on compared to every "by hand" calculation we've done.</p>
			<p><strong>Training:</strong> All of our training is done on the MNIST training set, as loaded from the <a href="https://huggingface.co/docs/datasets/index">Hugging Face: Datasets</a> python module. This dataset has 60k images, each ot 28x28 px, and each corresponding to a single digit, 0-9. During training, each model is first initialized and trained for one epoch. This epoch is timed. The model is then re-initialized and trained for a number of epochs corresponding to ~4 hours on a desktop computer that we have running in the background with an NVIDIA 3070 RTX graphics card. Accuracy on the validation set is recorded after each epoch of training.</p>
			<p><strong>Validation & Test:</strong> MNIST includes a 'test' dataset of 10k labeled images. We further split this up into an 8k "validation" set (which is used to generate the "Accuracy" axes in all our figures, except where otherwise noted, and a 2k "test" set (which we reserve for special use).</p>
			<p><strong>Random Seed:</strong> To ensure repeatability, we use the <a href="https://en.wikipedia.org/wiki/Taxicab_number">taxicab number</a> as our random seed, except where otherwise noted.</p>

		</article>

<!-- SECTION: ACTIVATION FUNCTIONS -->
		<article id="activation_functions" class="navTarget">
			<h3>Activation Functions</h3>

			<h4>Training Results From SimpleNet2</h4>
			<figure>
			<div class="overlapping_images">
			<object type="image/svg+xml" data="images/p_SimpleNet2_activationFxns_1729seed_dark.svg"
				 class="darkmode"
				 id="SN2_1729_dark"></object>
			<object type="image/svg+xml" data="images/p_SimpleNet2_activationFxns_1729seed_light.svg"
				 class="lightmode"
				 id="SN2_1729_light"></object>
			<object type="image/svg+xml" data="images/p_SimpleNet2_activationFxns_271828seed_dark.svg"
				 class="darkmode"
				 style="display:none"
				 id="SN2_271828_dark"></object>
			<object type="image/svg+xml" data="images/p_SimpleNet2_activationFxns_271828seed_light.svg"
				 class="lightmode"
				 style="display:none"
				 id="SN2_271828_light"></object>
			<object type="image/svg+xml" data="images/p_SimpleNet2_activationFxns_314159seed_dark.svg"
				 class="darkmode"
				 style="display:none"
				 id="SN2_314159_dark"></object>
			<object type="image/svg+xml" data="images/p_SimpleNet2_activationFxns_314159seed_light.svg"
				 class="lightmode"
				 style="display:none"
				 id="SN2_314159_light"></object>
			<figcaption>Random seed: <a id="SN2_1729" class="clickyHover" style="text-decoration:underline">1729</a> / <a id="SN2_271828" class="clickyHover">271828</a> / <a id="SN2_314159" class="clickyHover">314159</a></figcaption>
			</div>
			</figure>

			<h4>Training Results from LeNet5</h4>
			<figure>
			<div class="overlapping_images">
			<object type="image/svg+xml" data="images/p_LeNet5_activationFxns_1729seed_dark.svg"
				 class="darkmode"
				 id="LN5_1729_dark"></object>
			<object type="image/svg+xml" data="images/p_LeNet5_activationFxns_1729seed_light.svg"
				 class="lightmode"
				 id="LN5_1729_light"></object>
			<object type="image/svg+xml" data="images/p_LeNet5_activationFxns_271828seed_dark.svg"
				 class="darkmode"
				 style="display:none"
				 id="LN5_271828_dark"></object>
			<object type="image/svg+xml" data="images/p_LeNet5_activationFxns_271828seed_light.svg"
				 class="lightmode"
				 style="display:none"
				 id="LN5_271828_light"></object>
			<object type="image/svg+xml" data="images/p_LeNet5_activationFxns_314159seed_dark.svg"
				 class="darkmode"
				 style="display:none"
				 id="LN5_314159_dark"></object>
			<object type="image/svg+xml" data="images/p_LeNet5_activationFxns_314159seed_light.svg"
				 class="lightmode"
				 style="display:none"
				 id="LN5_314159_light"></object>
			<figcaption>Random seed: <a id="LN5_1729" class="clickyHover" style="text-decoration:underline">1729</a> / <a id="LN5_271828" class="clickyHover">271828</a> / <a id="LN5_314159" class="clickyHover">314159</a></figcaption>
			</div>
			</figure>

		</article>

<!-- END: ACTIVATION FUNCTIONS -->

<!-- SECTION: ABOUT -->
		<article id="about" class="navTarget">
			<h3>About</h3>
			<p>MNIST battleground is a repository of actual tests of deep learning techniques applied to, and compared on, accessible datasets. In particular, since different choices (of, e.g., activation function, NN architecture, hyperparameters, etc) lead to different computational costs, we will be comparing options by recording validation-set-accuracy vs FLOPs used during training.</p>
			<h3>Credit and Thanks</h3>
			<p>This site uses the <a href="https://github.com/be5invis/Iosevka">Iosevka</a> font, <a href="https://necolas.github.io/normalize.css/">Normalize.css</a>, and a remixed variant of <a href="https://simplecss.org/">Simple.css</a> using the <a href="https://github.com/morhetz/gruvbox">gruvbox</a> color scheme.</p>
		</article>
<!-- END: ABOUT -->

<!-- SECTION: CONTACT -->
		<article id="contact" class="navTarget">
			<h3>Contact</h3>
			<p>Feel free to reach us by email at <a href="mailto:battleground@mnist.org">battleground@mnist.org</a>.</p>
		</article>
<!-- END: CONTACT -->

	</main>

<!-- SECTION: SCRIPTS -->
	<script src="scripts/clickyHover3.js"></script>
	<script src="scripts/legend_items2.js"></script>
	<script src="scripts/set_scroll_margins.js"></script>
<!-- END: SCRIPTS -->

</body>

</html>
