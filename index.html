<!DOCTYPE html>
<html lang="en">

<head>
	<link rel="stylesheet" href="styles/normalize.css">
	<link rel="stylesheet" href="styles/simple.css">
	<link rel="stylesheet" href="utils.css">
	<meta name="viewport" content="width=device-width, initial-scale=1">


</head>

<body>
	<header>
		<h2>Welcome to MNIST Battleground</h2>
	</header>

<!-- SECTION: NAVBAR -->
	<nav class="navbar">
		<ul>
			<li><a href="#intro">Intro</a></li>
			<li><a href="#activation_functions">Activation Functions</a></li>
			<li><a href="#depth_of_architecture">Depth of Architecture</a></li>
			<li><a href="#methodology">Methodology</a></li>
			<li><a href="#architectures">Model Architectures</a></li>
			<li><a href="#about">About</a></li>
			<li><a href="#contact">Contact</a></li>
		</ul>
	</nav>
<!-- END: NAVBAR -->

	<main>

<!-- SECTION: INTRODUCTION -->
		<h3 id="intro" class="navTarget">Introduction</h3>
		<blockquote>
			<p>Deep Learning is full of choices:</p>
			<dl>
				<dd>What architecture to pick?</dd>
				<dd>What loss function?</dd>
				<dd>What learning rate?</dd>
				<dd>What activation functions?</dd>
				<dd>...and so on.</dd>
			</dl>
			<p>And many words have been spilt across the internet about all of these, but rarely are those words backed up with data.</p>
			<p><strong>That</strong> is what we're doing here.</p>
			<p>We're starting with a simple dataset that everyone should be familiar with: <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST</a>, and we'll be testing everything we can think of, and posting the results here. More data. Less fluff.</p>
			<p>In particular, we'll be presenting results in terms of Accuracy (out of 100% on the validation set) vs FLOPs spent on training. This later is a twist compared to the usual practice of counting training batches, but when different architecture choices may involve substantially different computational costs, it's important to have an apples-to-apples comparison.</p>
		</blockquote>
<!-- END: INTRODUCTION -->



<!-- SECTION: ACTIVATION FUNCTIONS -->
		<article id="activation_functions" class="navTarget">
			<h3>Activation Functions</h3>

			<h4>Training Results From <a href="#SimpleNet2Arch">SimpleNet2</a></h4>
			<figure>
			<div class="overlapping_images">
			<object type="image/svg+xml" data="images/p_SimpleNet2_activationFxns_1729seed_dark.svg"
				 class="darkmode startsActive"
				 id="SN2_1729_dark"></object>
			<object type="image/svg+xml" data="images/p_SimpleNet2_activationFxns_1729seed_light.svg"
				 class="lightmode startsActive"
				 id="SN2_1729_light"></object>
			<object type="image/svg+xml" data="images/p_SimpleNet2_activationFxns_271828seed_dark.svg"
				 class="darkmode"
				 style="display:none"
				 id="SN2_271828_dark"></object>
			<object type="image/svg+xml" data="images/p_SimpleNet2_activationFxns_271828seed_light.svg"
				 class="lightmode"
				 style="display:none"
				 id="SN2_271828_light"></object>
			<object type="image/svg+xml" data="images/p_SimpleNet2_activationFxns_314159seed_dark.svg"
				 class="darkmode"
				 style="display:none"
				 id="SN2_314159_dark"></object>
			<object type="image/svg+xml" data="images/p_SimpleNet2_activationFxns_314159seed_light.svg"
				 class="lightmode"
				 style="display:none"
				 id="SN2_314159_light"></object>
			<figcaption>Random seed: <a id="SN2_1729" class="clickyHover" style="text-decoration:underline">1729</a> / <a id="SN2_271828" class="clickyHover">271828</a> / <a id="SN2_314159" class="clickyHover">314159</a></figcaption>
			</div>
			</figure>

			<h4>Training Results from <a href="#LeNet5Arch">LeNet5</a></h4>
			<figure>
			<div class="overlapping_images">
			<object type="image/svg+xml" data="images/p_LeNet5_activationFxns_1729seed_dark.svg"
				 class="darkmode startsActive"
				 id="LN5_1729_dark"></object>
			<object type="image/svg+xml" data="images/p_LeNet5_activationFxns_1729seed_light.svg"
				 class="lightmode startsActive"
				 id="LN5_1729_light"></object>
			<object type="image/svg+xml" data="images/p_LeNet5_activationFxns_271828seed_dark.svg"
				 class="darkmode"
				 style="display:none"
				 id="LN5_271828_dark"></object>
			<object type="image/svg+xml" data="images/p_LeNet5_activationFxns_271828seed_light.svg"
				 class="lightmode"
				 style="display:none"
				 id="LN5_271828_light"></object>
			<object type="image/svg+xml" data="images/p_LeNet5_activationFxns_314159seed_dark.svg"
				 class="darkmode"
				 style="display:none"
				 id="LN5_314159_dark"></object>
			<object type="image/svg+xml" data="images/p_LeNet5_activationFxns_314159seed_light.svg"
				 class="lightmode"
				 style="display:none"
				 id="LN5_314159_light"></object>
			<figcaption>Random seed: <a id="LN5_1729" class="clickyHover" style="text-decoration:underline">1729</a> / <a id="LN5_271828" class="clickyHover">271828</a> / <a id="LN5_314159" class="clickyHover">314159</a></figcaption>
			</div>
			</figure>

		</article>

<!-- END: ACTIVATION FUNCTIONS -->

<!-- SECTION: DEPTH OF ARCHITECTURE -->
		<article id="depth_of_architecture" class="navTarget">
			<h3>Depth of Architecture</h3>
			<h4><a href="#SimpleNet2Arch">SimpleNet2</a> with varying number of 'middle layers'</h4>
			<figure>
			<div class="overlapping_images">
				<object type="image/svg+xml" data="images/p_SimpleNet2_0layers_dark.svg"
					class="darkmode startsActive"
					id="SN2_0layers_dark"></object>
				<object type="image/svg+xml" data="images/p_SimpleNet2_0layers_light.svg"
					class="lightmode startsActive"
					id="SN2_0layers_light"></object>
				<object type="image/svg+xml" data="images/p_SimpleNet2_1layers_dark.svg"
					class="darkmode"
					style="display:none"
					id="SN2_1layers_dark"></object>
				<object type="image/svg+xml" data="images/p_SimpleNet2_1layers_light.svg"
					class="lightmode"
					style="display:none"
					id="SN2_1layers_light"></object>
				<object type="image/svg+xml" data="images/p_SimpleNet2_2layers_dark.svg"
					class="darkmode"
					style="display:none"
					id="SN2_2layers_dark"></object>
				<object type="image/svg+xml" data="images/p_SimpleNet2_2layers_light.svg"
					class="lightmode"
					style="display:none"
					id="SN2_2layers_light"></object>
				<object type="image/svg+xml" data="images/p_SimpleNet2_3layers_dark.svg"
					class="darkmode"
					style="display:none"
					id="SN2_3layers_dark"></object>
				<object type="image/svg+xml" data="images/p_SimpleNet2_3layers_light.svg"
					class="lightmode"
					style="display:none"
					id="SN2_3layers_light"></object>
				<object type="image/svg+xml" data="images/p_SimpleNet2_4layers_dark.svg"
					class="darkmode"
					style="display:none"
					id="SN2_4layers_dark"></object>
				<object type="image/svg+xml" data="images/p_SimpleNet2_4layers_light.svg"
					class="lightmode"
					style="display:none"
					id="SN2_4layers_light"></object>
				<object type="image/svg+xml" data="images/p_SimpleNet2_5layers_dark.svg"
					class="darkmode"
					style="display:none"
					id="SN2_5layers_dark"></object>
				<object type="image/svg+xml" data="images/p_SimpleNet2_5layers_light.svg"
					class="lightmode"
					style="display:none"
					id="SN2_5layers_light"></object>
				<object type="image/svg+xml" data="images/p_SimpleNet2_10layers_dark.svg"
					class="darkmode"
					style="display:none"
					id="SN2_10layers_dark"></object>
				<object type="image/svg+xml" data="images/p_SimpleNet2_10layers_light.svg"
					class="lightmode"
					style="display:none"
					id="SN2_10layers_light"></object>
			<figcaption>Depth of architecture: <a id="SN2_0layers" class="clickyHover" style="text-decoration:underline">0</a> / <a id="SN2_1layers" class="clickyHover">1</a> / <a id="SN2_2layers" class="clickyHover">2</a> / <a id="SN2_3layers" class="clickyHover">3</a> / <a id="SN2_4layers" class="clickyHover">4</a> / <a id="SN2_5layers" class="clickyHover">5</a> / <a id="SN2_10layers" class="clickyHover">10</a></figcaption>
			</div>
			</figure>

			<figure>
			<div class="overlapping_images">
				<object type="image/svg+xml" data="images/p_SimpleNet2_Xlayers_ReLU_dark.svg"
					class="darkmode startsActive"
					id="SN2_Xlayers_ReLU_dark"></object>
				<object type="image/svg+xml" data="images/p_SimpleNet2_Xlayers_ReLU_light.svg"
					class="lightmode startsActive"
					id="SN2_Xlayers_ReLU_light"></object>
				<object type="image/svg+xml" data="images/p_SimpleNet2_Xlayers_ELU_dark.svg"
					class="darkmode"
					style="display:none"
					id="SN2_Xlayers_ELU_dark"></object>
				<object type="image/svg+xml" data="images/p_SimpleNet2_Xlayers_ELU_light.svg"
					class="lightmode"
					style="display:none"
					id="SN2_Xlayers_ELU_light"></object>
				<object type="image/svg+xml" data="images/p_SimpleNet2_Xlayers_LeakyReLU_dark.svg"
					class="darkmode"
					style="display:none"
					id="SN2_Xlayers_LeakyReLU_dark"></object>
				<object type="image/svg+xml" data="images/p_SimpleNet2_Xlayers_LeakyReLU_light.svg"
					class="lightmode"
					style="display:none"
					id="SN2_Xlayers_LeakyReLU_light"></object>
				<object type="image/svg+xml" data="images/p_SimpleNet2_Xlayers_PReLU_dark.svg"
					class="darkmode"
					style="display:none"
					id="SN2_Xlayers_PReLU_dark"></object>
				<object type="image/svg+xml" data="images/p_SimpleNet2_Xlayers_PReLU_light.svg"
					class="lightmode"
					style="display:none"
					id="SN2_Xlayers_PReLU_light"></object>
				<object type="image/svg+xml" data="images/p_SimpleNet2_Xlayers_SiLU_dark.svg"
					class="darkmode"
					style="display:none"
					id="SN2_Xlayers_SiLU_dark"></object>
				<object type="image/svg+xml" data="images/p_SimpleNet2_Xlayers_SiLU_light.svg"
					class="lightmode"
					style="display:none"
					id="SN2_Xlayers_SiLU_light"></object>
				<object type="image/svg+xml" data="images/p_SimpleNet2_Xlayers_sigmoid_dark.svg"
					class="darkmode"
					style="display:none"
					id="SN2_Xlayers_sigmoid_dark"></object>
				<object type="image/svg+xml" data="images/p_SimpleNet2_Xlayers_sigmoid_light.svg"
					class="lightmode"
					style="display:none"
					id="SN2_Xlayers_sigmoid_light"></object>
				<object type="image/svg+xml" data="images/p_SimpleNet2_Xlayers_tanh_dark.svg"
					class="darkmode"
					style="display:none"
					id="SN2_Xlayers_tanh_dark"></object>
				<object type="image/svg+xml" data="images/p_SimpleNet2_Xlayers_tanh_light.svg"
					class="lightmode"
					style="display:none"
					id="SN2_Xlayers_tanh_light"></object>
			<figcaption>Activation function: <a id="SN2_Xlayers_ReLU" class="clickyHover" style="text-decoration:underline">ReLU</a> / <a id="SN2_Xlayers_ELU" class="clickyHover">ELU</a> / <a id="SN2_Xlayers_LeakyReLU" class="clickyHover">LeakyReLU</a> / <a id="SN2_Xlayers_PReLU" class="clickyHover">PReLU</a> / <a id="SN2_Xlayers_SiLU" class="clickyHover">SiLU</a> / <a id="SN2_Xlayers_sigmoid" class="clickyHover">sigmoid</a> / <a id="SN2_Xlayers_tanh" class="clickyHover">tanh</a></figcaption>
				
			</figure>
		</article>

<!-- END: DEPTH OF ARCHITECTURE -->

<!-- SECTION: METHODOLOGY -->
<article id="methodology" class="navTarget">
	<h3>Methodology</h3>
	<p><strong>Philosophy:</strong> Our philosophy is to start with the simplest and easiest things we can do first, and iterate from there.</p>
	<p><strong>FLOPs:</strong> We report our results in terms of FLOPs spent on training--rather than batch number--because it gives a more fair comparison between different model architectures. We obtain our FLOPs numbers by using the <a href="https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html">PyTorch Profiler</a> tool, which produces FLOPs estimates that so far have landed spot on compared to every "by hand" calculation we've done.</p>
	<p><strong>Training:</strong> All of our training is done on the MNIST training set, as loaded from the <a href="https://huggingface.co/docs/datasets/index">Hugging Face: Datasets</a> python module. This dataset has 60k images, each ot 28x28 px, and each corresponding to a single digit, 0-9. During training, each model is first initialized and trained for one epoch. This epoch is timed. The model is then re-initialized and trained for a number of epochs corresponding to ~4 hours on a desktop computer that we have running in the background with an NVIDIA 3070 RTX graphics card. Accuracy on the validation set is recorded after each epoch of training.</p>
	<p><strong>Validation & Test:</strong> MNIST includes a 'test' dataset of 10k labeled images. We further split this up into an 8k "validation" set (which is used to generate the "Accuracy" axes in all our figures, except where otherwise noted), and a 2k "test" set (which we reserve for special use).</p>
	<p><strong>Batch Size:</strong> All models are trained at a default batch size of 100 items, unless otherwise noted.</p>
	<p><strong>Optimizer:</strong>All models are trained using the <a href="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html">torch.optim.SGD</a> optimizer, unless otherwise noted. We use pytorch defaults for all kwargs unless otherwise noted.</p>
	<p><strong>Learning Rate:</strong>All models are trained using a learning rate of 1e-3, unless otherwise noted.</p>
	<p><strong>Loss Function:</strong>All models are trained using <a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html">cross entropy loss</a>, unless otherwise noted.</p>
	<p><strong>Random Seed:</strong> To ensure repeatability, we use the <a href="https://en.wikipedia.org/wiki/Taxicab_number">taxicab number</a> as our random seed, except where otherwise noted.</p>

</article>
<!-- END: METHODOLOGY -->

<!-- SECTION: ARCHITECTURES -->

<article id="architectures" class="navTarget">
	<h3>Model Architectures</h3>
	<h4 id="SimpleNet2Arch">SimpleNet2</h4>
	<div class="overlapping_images">
		<img class="darkmode" src="images/architectures/SimpleNet2_arch_dark.png">
		<img class="lightmode" src="images/architectures/SimpleNet2_arch_light.png">
	</div>
	<p><a href="models/m_SimpleNet2_ReLU.py">[pytorch implementation]</a></p>
	<p>SimpleNet2 is the simplest possible neural net capable of hosting an activation function.</p>
	<p>In the base variant, which is the one used, for example, in the <a href="#activation_functions">activation functions investigation</a>, N = 0. Which is to say that in the base variant, there are no "middle" layers, and the model consists of just a single, fully-connected, layer of 100 neurons in between the input pixels and the output logits.</p>
	<p>In the <a href="#depth_of_architecture">depth of architecture investigation</a>, N is set to different values, as labeled in the graph legends. For example, "m_SimpleNet2_ReLU" refers to the base model, with no middle layers, and using the ReLU activation function. "m_SimpleNet2_3layers_ReLU" refers to the SimpleNet2 model with three middle layers (so a total of 4 fully-connected layers... 1 initial + 3 middle), with all layers using the ReLU activation function.</p>
	<h4 id="LeNet5Arch">LeNet5</h4>
	<p><a href="models/m_LeNet5.py">[pytorch implementation]</a></p>
	<p>LeNet5 is a re-implementation of the original <a href="https://en.wikipedia.org/wiki/LeNet">LeNet-5 neural net</a>.</p>

</article>

<!-- END: ARCHITECTURES -->

<!-- SECTION: ABOUT -->
		<article id="about" class="navTarget">
			<h3>About</h3>
			<p>MNIST battleground is a repository of actual tests of deep learning techniques applied to, and compared on, accessible datasets. In particular, since different choices (of, e.g., activation function, NN architecture, hyperparameters, etc) lead to different computational costs, we will be comparing options by recording validation-set-accuracy vs FLOPs used during training.</p>
			<h3>Credit and Thanks</h3>
			<p>First off, credit and thanks go to everyone who has come before us, both great and small. Deep Learning is a fascinating space, and every contribution, from groundbreaking papers that usher in new eras, to humble blog posts that educate on one point or another, is noticed and valued.</p>
			<p>This site uses the <a href="https://github.com/be5invis/Iosevka">Iosevka</a> font, <a href="https://necolas.github.io/normalize.css/">Normalize.css</a>, and a remixed variant of <a href="https://simplecss.org/">Simple.css</a> using the <a href="https://github.com/morhetz/gruvbox">gruvbox</a> color scheme.</p>
		</article>
<!-- END: ABOUT -->

<!-- SECTION: CONTACT -->
		<article id="contact" class="navTarget">
			<h3>Contact</h3>
			<p>Feel free to reach us by email at <a href="mailto:battleground@mnist.org">battleground@mnist.org</a>.</p>
		</article>
<!-- END: CONTACT -->

	</main>

	<div class="footerbar">
		<div class="footer-content">
		<p>All graphs are interactive. Try clicking on legend items to highlight individual traces.</p>
		<p>This website is optimized for Chrome on desktop. Other browsers may experience reduced functionality.</p>
		</div>
		<button class="close-button">X</button>
	</div>

<!-- SECTION: SCRIPTS -->
	<script src="scripts/legend_items3.js"></script>
	<script src="scripts/clickyHover4.js"></script>
	<script src="scripts/set_scroll_margins.js"></script>
	<script src="scripts/footer_control.js"></script>
<!-- END: SCRIPTS -->

</body>

</html>
